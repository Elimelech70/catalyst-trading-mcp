# Free Data Sources & ML-Optimized Storage Strategy

**Name of Application**: Catalyst Trading System  
**Name of file**: free-data-sources-ml-storage-strategy.md  
**Version**: 1.0.0  
**Last Updated**: 2025-10-25  
**Purpose**: Identify freely available data sources and optimize storage for ML training  
**Philosophy**: Store decisions/outcomes + references, not raw data (download on-demand for ML)

---

## REVISION HISTORY

**v1.0.0 (2025-10-25)** - Free Data Analysis & Reference-Based Storage
- Complete analysis of free vs paid data sources
- Reference-based storage strategy for ML training
- On-demand data retrieval architecture
- Massive storage cost reduction (200GB â†’ 5GB)
- No loss of ML training capability

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Free Data Sources Available](#2-free-data-sources-available)
3. [Reference-Based Storage Strategy](#3-reference-based-storage-strategy)
4. [Storage Comparison: Full vs Reference-Based](#4-storage-comparison-full-vs-reference-based)
5. [ML Training Workflow](#5-ml-training-workflow)
6. [Implementation Architecture](#6-implementation-architecture)
7. [Rate Limits & Scheduling](#7-rate-limits--scheduling)
8. [Recommendations](#8-recommendations)

---

## 1. Executive Summary

### 1.1 Core Insight

**You're absolutely right** - most market data is freely available from multiple sources. Instead of storing 200GB of raw OHLCV data, we can:

```yaml
Store (Small):
  âœ… Trading decisions (what we chose, why)
  âœ… Outcomes (win/loss, P&L, R:R achieved)
  âœ… References (symbol, timestamp, source)
  âœ… Composite scores (our calculations)
  
Don't Store (Large):
  âŒ Raw OHLCV data (download from Alpaca/Polygon when needed)
  âŒ Full news articles (reference by URL, fetch when needed)
  âŒ Technical indicators (recalculate from OHLCV on-demand)
```

**Result**: 
- Storage: 200GB â†’ **5GB** (40x reduction!)
- ML capability: **NO LOSS** (data still accessible)
- Cost: $120/month â†’ **$15/month** (8x savings)

### 1.2 What This Means

```yaml
Traditional Approach (What we planned):
  - Store everything locally
  - 200GB database
  - $120/month DigitalOcean
  - Instant ML training (data already local)

Reference-Based Approach (Better):
  - Store decisions + references
  - 5GB database  
  - $15/month DigitalOcean
  - ML training: download data on-demand (adds 10-30 min)
  - Net: 8x cheaper, same ML capability
```

**Trade-off**: ML training takes 10-30 minutes longer (to download historical data), but you save $1,260/year.

---

## 2. Free Data Sources Available

### 2.1 Price Data (OHLCV) - 100% FREE

#### **Alpaca Markets (Current Broker)**
```yaml
Source: Alpaca Data API v2
Access: FREE with account (no subscription required)
Historical: Unlimited lookback (all US stocks)
Real-time: 15-minute delay (free), real-time requires subscription

Data Available:
  - OHLCV bars: 1min, 5min, 15min, 30min, 1hour, 1day
  - All US stocks: NYSE, NASDAQ, AMEX
  - Historical: Back to 2016+ for most stocks
  - Format: JSON via REST API
  
Rate Limits:
  - 200 requests/minute (free tier)
  - 1,000 symbols per request
  
Cost: $0 (FREE forever)

API Endpoint:
  GET https://data.alpaca.markets/v2/stocks/{symbol}/bars
  Parameters: timeframe, start, end, limit
```

**Example**: Get 5 years of AAPL 5-minute bars
```python
import requests

url = "https://data.alpaca.markets/v2/stocks/AAPL/bars"
params = {
    "timeframe": "5Min",
    "start": "2020-01-01",
    "end": "2025-01-01",
    "limit": 10000
}
headers = {
    "APCA-API-KEY-ID": "your_key",
    "APCA-API-SECRET-KEY": "your_secret"
}

response = requests.get(url, params=params, headers=headers)
data = response.json()

# Result: ~262,000 bars (5 years Ã— 252 days Ã— 78 bars/day Ã— 5 splits)
# Download time: ~5 minutes (paginated)
# Storage if saved: ~60MB compressed
# Our approach: Don't save, reference only
```

#### **Yahoo Finance (Backup Source)**
```yaml
Source: yfinance Python library
Access: FREE, no API key required
Historical: Unlimited lookback (decades)
Real-time: 15-minute delay

Data Available:
  - OHLCV: 1min, 2min, 5min, 15min, 30min, 1hour, 1day, 1week
  - Dividends, splits, fundamentals
  - All US + international stocks
  
Rate Limits:
  - ~2,000 requests/hour (unofficial)
  - No official API, scraping-based
  
Cost: $0 (FREE)

Usage:
  pip install yfinance
  import yfinance as yf
  data = yf.download("AAPL", start="2020-01-01", end="2025-01-01", interval="5m")
```

#### **Polygon.io (Premium Alternative)**
```yaml
Source: Polygon.io API
Access: FREE tier available (limited), $25/month (basic), $99/month (pro)
Historical: Unlimited with subscription

Free Tier:
  - 5 API calls/minute
  - 2 years historical data
  - Enough for occasional ML training
  
Paid Tier ($25/month):
  - 100 requests/minute
  - Unlimited historical
  - Real-time data included
  
Cost: $0 (free tier) or $25/month (basic)
```

**Verdict**: Alpaca free tier is sufficient for all our needs. Polygon free tier works as backup.

---

### 2.2 News Data - MIXED (Some Free, Some Paid)

#### **Free News Sources**

**1. Alpaca News API (Integrated with Broker)**
```yaml
Source: Alpaca News API (powered by Benzinga/Refinitiv)
Access: FREE with Alpaca account
Coverage: Real-time news for all US stocks
Historical: Last 30 days (free)

Data Available:
  - Headlines, summaries, URLs
  - Symbols mentioned
  - Timestamp
  
Rate Limits:
  - 200 requests/minute
  - 50 articles per request
  
Cost: $0 (FREE with trading account)

Limitations:
  - Only 30 days historical (not great for ML)
  - No full article text
  - No sentiment scores (must calculate ourselves)
```

**2. NewsAPI.org**
```yaml
Source: NewsAPI.org
Access: FREE tier available (limited)
Coverage: 80,000+ sources worldwide

Free Tier:
  - 100 requests/day
  - Last 1 month historical
  - No commercial use
  
Developer Tier ($449/month):
  - Unlimited requests
  - Full historical archive
  - Commercial use allowed
  
Cost: $0 (free) or $449/month (way too expensive)

Verdict: Use free tier for supplemental news, not primary
```

**3. Yahoo Finance RSS Feeds**
```yaml
Source: Yahoo Finance RSS
Access: FREE, no authentication
Coverage: Major stocks only

Data Available:
  - Headlines, URLs
  - Published date
  - No full text
  
Cost: $0 (FREE)

Limitation: No historical archive, real-time only
```

**4. SEC EDGAR Filings (Regulatory News)**
```yaml
Source: SEC EDGAR
Access: FREE, public data
Coverage: All public US companies

Data Available:
  - 8-K filings (material events)
  - 10-Q/10-K (quarterly/annual reports)
  - Form 4 (insider trading)
  - Timestamps, full text
  
Cost: $0 (FREE forever, government data)

Perfect for:
  - Earnings announcements
  - Material events
  - Insider trading analysis
```

#### **Paid News Sources (Better Quality)**

**Benzinga Pro**
```yaml
Cost: $99/month (basic) to $499/month (pro)
Coverage: Real-time breaking news, multi-year historical
Quality: High (professional journalists)

Verdict: Wait until profitable (Month 6+)
```

**Bloomberg Terminal**
```yaml
Cost: $2,000/month (!!!)
Verdict: Never (way overkill for our needs)
```

#### **News Strategy**
```yaml
Phase 1 (Months 0-6): FREE sources only
  - Alpaca News API (primary)
  - Yahoo Finance RSS (supplemental)
  - SEC EDGAR (regulatory events)
  
Phase 2 (Months 6+): Add Benzinga if profitable
  - Benzinga Pro: $99/month
  - Better historical data for ML
```

---

### 2.3 Economic Data - 100% FREE

#### **FRED (Federal Reserve Economic Data)**
```yaml
Source: Federal Reserve Bank of St. Louis
Access: FREE API (requires free API key)
Coverage: 816,000+ economic time series

Data Available:
  - Federal Funds Rate (DFF)
  - Treasury Yields (DGS10, T10Y2Y)
  - CPI (CPIAUCSL)
  - Unemployment (UNRATE)
  - GDP (GDP)
  - VIX (VIXCLS)
  - Hundreds of thousands more
  
Historical: Often 50+ years back
Update Frequency: Daily, weekly, monthly, quarterly
Rate Limits: 120 requests/minute (free)

Cost: $0 (FREE forever)

API:
  https://api.stlouisfed.org/fred/series/observations
  ?series_id=DFF&api_key=YOUR_KEY
```

**Perfect for ML**: Macro economic context for trades

---

### 2.4 Technical Indicators - FREE (Calculate Ourselves)

```yaml
All technical indicators are derived from OHLCV data:
  - Moving averages (SMA, EMA)
  - RSI, MACD, ATR, Bollinger Bands
  - Volume analysis (OBV, volume ratio)
  - Support/resistance levels
  
Cost: $0 (just CPU time to calculate)

Libraries:
  - TA-Lib (free, open source)
  - Pandas-TA (free)
  - NumPy/SciPy (free)
```

**Strategy**: Never store technical indicators, recalculate on-demand from OHLCV

---

### 2.5 Fundamentals Data - MIXED

```yaml
Free Sources:
  - Yahoo Finance (basic fundamentals)
  - SEC EDGAR (official filings)
  - Alpaca (some fundamentals via API)
  
Paid Sources:
  - Financial Modeling Prep: $15/month (basic)
  - Alpha Vantage: $50/month
  - Quandl: $50+/month
  
Our Need: LOW (we're day trading, not fundamental investing)
Strategy: Use free sources if needed, likely not critical for Stage 1
```

---

## 3. Reference-Based Storage Strategy

### 3.1 Core Principle

**Store WHAT WE DID and WHY, not the RAW DATA**

```yaml
Traditional Storage (200GB):
  trading_history table:
    - history_id, security_id, time_id
    - open, high, low, close, volume
    - vwap, trade_count
    â†’ 342,900,000 rows Ã— 120 bytes = 39.1 GB
    
  news_sentiment table:
    - news_id, security_id, time_id
    - headline, summary, url, source
    - sentiment_score, sentiment_label
    â†’ 252,000 rows Ã— 2 KB = 480 MB
    
  technical_indicators table:
    - indicator_id, security_id, time_id
    - sma_20, sma_50, sma_200, ema_9, ema_21
    - rsi_14, macd, atr_14, etc.
    â†’ 22,050,000 rows Ã— 300 bytes = 6.3 GB
    
Total: ~46 GB of "downloaded" data

Reference-Based Storage (5GB):
  trading_decisions table:
    - decision_id, cycle_id, security_id
    - decision_timestamp
    - data_source_reference (Alpaca API endpoint)
    - composite_score, selected (true/false)
    - reason_rejected (if not selected)
    â†’ 1,890,000 rows Ã— 500 bytes = 900 MB
    
  trade_outcomes table:
    - trade_id, security_id, pattern
    - entry_timestamp, exit_timestamp  
    - entry_price, exit_price, stop_loss, take_profit
    - realized_pnl, r_multiple, win (true/false)
    - data_source_reference
    â†’ 6,300 rows Ã— 1 KB = 6.3 MB
    
  news_references table:
    - news_ref_id, security_id, timestamp
    - source (Alpaca/Yahoo/SEC), article_url
    - catalyst_type, sentiment_label
    - price_impact_5min, price_impact_1h
    â†’ 252,000 rows Ã— 200 bytes = 48 MB
    
Total: ~1 GB of "decision" data
```

### 3.2 What We Store vs Reference

| Data Type | Traditional | Reference-Based | ML Impact |
|-----------|-------------|-----------------|-----------|
| **OHLCV Data** | STORE (39GB) | REFERENCE â†’ Alpaca API | None (download on-demand) |
| **News Articles** | STORE (480MB) | REFERENCE â†’ URLs + metadata | None (fetch on-demand) |
| **Technical Indicators** | STORE (6.3GB) | REFERENCE â†’ Recalculate from OHLCV | None (calculate on-demand) |
| **Scan Results** | STORE (900MB) | **STORE** (critical decisions) | KEEP (essential for ML) |
| **Trade Outcomes** | STORE (6MB) | **STORE** (ground truth) | KEEP (essential for ML) |
| **Economic Data** | STORE (50MB) | REFERENCE â†’ FRED API | None (download on-demand) |

### 3.3 Detailed Table Design

#### **trading_decisions** (STORE - Critical for ML)
```sql
CREATE TABLE trading_decisions (
    decision_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    cycle_id UUID NOT NULL REFERENCES trading_cycles(cycle_id),
    security_id INTEGER NOT NULL REFERENCES securities(security_id),
    
    -- Decision Context
    decision_timestamp TIMESTAMPTZ NOT NULL,
    decision_phase VARCHAR(50) NOT NULL,  -- 'scan', 'news_filter', 'pattern', 'technical', 'risk'
    
    -- OUR DECISION (What we chose)
    selected BOOLEAN NOT NULL,
    composite_score DECIMAL(6,3),
    
    -- Component Scores (What we calculated)
    news_catalyst_score DECIMAL(6,3),
    pattern_confidence DECIMAL(6,3),
    technical_score DECIMAL(6,3),
    risk_score DECIMAL(6,3),
    
    -- Why Rejected (If not selected)
    rejection_reason VARCHAR(200),
    
    -- Data Source References (Where to get raw data for ML)
    ohlcv_source JSONB DEFAULT '{
        "api": "alpaca",
        "endpoint": "https://data.alpaca.markets/v2/stocks/{symbol}/bars",
        "timeframe": "5Min",
        "start": "{timestamp - 1 hour}",
        "end": "{timestamp}"
    }',
    
    news_sources JSONB DEFAULT '[]',  -- Array of news article URLs
    
    -- ML Training Labels (Ground truth - what happened after)
    actual_price_change_1h DECIMAL(8,4),
    actual_price_change_4h DECIMAL(8,4),
    actual_price_change_1d DECIMAL(8,4),
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_decisions_cycle ON trading_decisions(cycle_id);
CREATE INDEX idx_decisions_selected ON trading_decisions(selected);
CREATE INDEX idx_decisions_timestamp ON trading_decisions(decision_timestamp DESC);

COMMENT ON TABLE trading_decisions IS 'OUR DECISIONS - what we chose and why';
COMMENT ON COLUMN trading_decisions.ohlcv_source IS 'Reference to free Alpaca API for ML retrieval';
```

**Size**: 1,890,000 decisions Ã— 500 bytes = **945 MB**

#### **trade_outcomes** (STORE - Ground Truth for ML)
```sql
CREATE TABLE trade_outcomes (
    trade_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    position_id UUID NOT NULL REFERENCES positions(position_id),
    security_id INTEGER NOT NULL REFERENCES securities(security_id),
    
    -- Trade Details
    pattern VARCHAR(50) NOT NULL,
    entry_timestamp TIMESTAMPTZ NOT NULL,
    exit_timestamp TIMESTAMPTZ NOT NULL,
    
    entry_price DECIMAL(12,4) NOT NULL,
    exit_price DECIMAL(12,4) NOT NULL,
    stop_loss DECIMAL(12,4),
    take_profit DECIMAL(12,4),
    
    -- Outcome (Ground Truth)
    realized_pnl DECIMAL(12,2) NOT NULL,
    realized_pnl_pct DECIMAL(8,4) NOT NULL,
    r_multiple DECIMAL(6,3),
    win BOOLEAN NOT NULL,
    
    -- Market Context References
    entry_context JSONB DEFAULT '{
        "ohlcv_source": "alpaca",
        "news_urls": [],
        "vix": null,
        "spy_price": null
    }',
    
    -- What We Learned
    lessons_learned TEXT,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_outcomes_security ON trade_outcomes(security_id);
CREATE INDEX idx_outcomes_pattern ON trade_outcomes(pattern);
CREATE INDEX idx_outcomes_win ON trade_outcomes(win);

COMMENT ON TABLE trade_outcomes IS 'GROUND TRUTH - actual trade results for ML training';
```

**Size**: 6,300 trades Ã— 1 KB = **6.3 MB**

#### **news_references** (STORE metadata, REFERENCE full text)
```sql
CREATE TABLE news_references (
    news_ref_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    security_id INTEGER NOT NULL REFERENCES securities(security_id),
    
    published_at TIMESTAMPTZ NOT NULL,
    source VARCHAR(100) NOT NULL,  -- 'Alpaca', 'Yahoo', 'SEC'
    
    -- Metadata ONLY (not full text)
    headline VARCHAR(500),
    article_url TEXT NOT NULL,  -- Reference to fetch full text
    
    -- OUR ANALYSIS (store this)
    catalyst_type VARCHAR(50),
    sentiment_label VARCHAR(20),
    sentiment_score DECIMAL(4,3),
    
    -- Price Impact (Actual - store this)
    price_impact_5min DECIMAL(6,3),
    price_impact_1h DECIMAL(6,3),
    price_impact_4h DECIMAL(6,3),
    
    -- Source Reliability (Learn over time)
    source_reliability_score DECIMAL(4,3) DEFAULT 0.500,
    
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_news_ref_security ON news_references(security_id);
CREATE INDEX idx_news_ref_published ON news_references(published_at DESC);

COMMENT ON TABLE news_references IS 'News metadata + URL reference (full text downloaded on-demand for ML)';
COMMENT ON COLUMN news_references.article_url IS 'Fetch full article text from this URL when needed for ML';
```

**Size**: 252,000 articles Ã— 200 bytes = **48 MB**

---

## 4. Storage Comparison: Full vs Reference-Based

### 4.1 5-Year Storage Projection

| Table | Full Storage | Reference Storage | Savings |
|-------|--------------|-------------------|---------|
| **OHLCV Data** | 39.1 GB | 0 GB (reference only) | 39.1 GB |
| **News Articles** | 480 MB | 48 MB (metadata only) | 432 MB |
| **Technical Indicators** | 6.3 GB | 0 GB (calculate on-demand) | 6.3 GB |
| **Scan Results** | 900 MB | 945 MB (keep decisions) | -45 MB |
| **Trade Outcomes** | 6.3 MB | 6.3 MB (keep ground truth) | 0 MB |
| **Performance Metrics** | 2.5 MB | 2.5 MB (keep) | 0 MB |
| **Economic Data** | 50 MB | 0 MB (FRED API reference) | 50 MB |
| **Indexes/Overhead** | 10 GB | 500 MB | 9.5 GB |
| **TOTAL** | **57.7 GB** | **1.5 GB** | **56.2 GB** |

**With Growth Factor (more securities over time)**:
- Full Storage (Year 5): ~200 GB
- Reference Storage (Year 5): ~5 GB
- **Savings: 195 GB (97.5% reduction!)**

### 4.2 Cost Comparison

| Approach | Year 1 | Year 3 | Year 5 | Total 5-Year |
|----------|--------|--------|--------|--------------|
| **Full Storage (DigitalOcean)** | $180 | $540 | $1,440 | $7,200 |
| **Reference Storage (DigitalOcean)** | $180 | $180 | $180 | $900 |
| **Savings** | $0 | $360 | $1,260 | $6,300 |

Plus you can stay on the Basic $15/month plan forever!

---

## 5. ML Training Workflow

### 5.1 Traditional Workflow (Full Storage)

```python
# ML Training: Query local database (instant)
async def train_pattern_recognition_model():
    # All data already in database
    features = await db.fetch("""
        SELECT 
            s.symbol,
            th.close, th.volume,
            ti.rsi_14, ti.macd, ti.atr_14,
            ns.sentiment_score,
            sd.composite_score,
            to.win as label
        FROM scan_decisions sd
        JOIN trading_history th ON th.security_id = sd.security_id
        JOIN technical_indicators ti ON ti.security_id = sd.security_id
        JOIN news_sentiment ns ON ns.security_id = sd.security_id
        JOIN trade_outcomes to ON to.security_id = sd.security_id
        WHERE sd.created_at >= NOW() - INTERVAL '5 years'
    """)
    
    # Train model
    model = train_model(features)
    
    # Total time: 5 minutes (all data local)
```

### 5.2 Reference-Based Workflow (Smart Download)

```python
# ML Training: Download on-demand (10-30 min first time, cached after)
async def train_pattern_recognition_model():
    # Step 1: Get our decisions from database (instant)
    decisions = await db.fetch("""
        SELECT 
            decision_id,
            security_id,
            decision_timestamp,
            composite_score,
            selected,
            ohlcv_source,
            news_sources
        FROM trading_decisions
        WHERE created_at >= NOW() - INTERVAL '5 years'
    """)
    
    # Step 2: Download OHLCV data from Alpaca (10-20 min, one-time)
    ohlcv_cache = {}
    for decision in decisions:
        symbol = get_symbol(decision.security_id)
        
        # Check local cache first
        if symbol not in ohlcv_cache:
            # Download from Alpaca (free)
            ohlcv_cache[symbol] = await download_alpaca_ohlcv(
                symbol=symbol,
                timeframe="5Min",
                start=decision.decision_timestamp - timedelta(hours=1),
                end=decision.decision_timestamp + timedelta(hours=4)
            )
    
    # Step 3: Calculate technical indicators (5 min, CPU)
    for symbol, ohlcv in ohlcv_cache.items():
        ohlcv['rsi'] = calculate_rsi(ohlcv['close'])
        ohlcv['macd'] = calculate_macd(ohlcv['close'])
        ohlcv['atr'] = calculate_atr(ohlcv)
    
    # Step 4: Fetch news full text if needed (5-10 min, network)
    news_cache = {}
    for decision in decisions:
        for news_url in decision.news_sources:
            if news_url not in news_cache:
                news_cache[news_url] = await fetch_article_text(news_url)
    
    # Step 5: Join everything together (instant)
    features = []
    for decision in decisions:
        symbol = get_symbol(decision.security_id)
        timestamp = decision.decision_timestamp
        
        features.append({
            'close': ohlcv_cache[symbol].loc[timestamp]['close'],
            'volume': ohlcv_cache[symbol].loc[timestamp]['volume'],
            'rsi': ohlcv_cache[symbol].loc[timestamp]['rsi'],
            'macd': ohlcv_cache[symbol].loc[timestamp]['macd'],
            'atr': ohlcv_cache[symbol].loc[timestamp]['atr'],
            'composite_score': decision.composite_score,
            'selected': decision.selected
        })
    
    # Step 6: Train model
    model = train_model(features)
    
    # Total time: 
    #   - First run: 20-30 minutes (download + calculate)
    #   - Subsequent runs: 5 minutes (use cached data)
```

### 5.3 Smart Caching Strategy

```python
# Cache downloaded data locally for repeated ML training
# Store in `/tmp/ml_cache/` or `/home/claude/ml_training_cache/`

CACHE_DIR = "/home/claude/ml_training_cache"

async def download_alpaca_ohlcv_cached(symbol, start, end):
    """Download OHLCV with local file caching"""
    cache_key = f"{symbol}_{start}_{end}.parquet"
    cache_path = Path(CACHE_DIR) / cache_key
    
    # Check if cached
    if cache_path.exists():
        # Use cached data (instant)
        return pd.read_parquet(cache_path)
    
    # Not cached - download from Alpaca
    data = await download_from_alpaca_api(symbol, start, end)
    
    # Save to cache for next time
    data.to_parquet(cache_path)
    
    return data

# Result: 
#   - First ML training: 20-30 min
#   - Second ML training: 5 min (all cached)
#   - Cache size: ~500MB (way smaller than 200GB database!)
```

---

## 6. Implementation Architecture

### 6.1 System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PRODUCTION TRADING SYSTEM                  â”‚
â”‚  (DigitalOcean - 5GB database)                         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  PostgreSQL Database (5GB)            â”‚             â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚  â”‚  âœ… trading_decisions (945MB)        â”‚             â”‚
â”‚  â”‚  âœ… trade_outcomes (6MB)             â”‚             â”‚
â”‚  â”‚  âœ… news_references (48MB)           â”‚             â”‚
â”‚  â”‚  âœ… securities, sectors, time_dim    â”‚             â”‚
â”‚  â”‚  âœ… trading_cycles, positions        â”‚             â”‚
â”‚  â”‚  âŒ NO OHLCV data                    â”‚             â”‚
â”‚  â”‚  âŒ NO technical indicators          â”‚             â”‚
â”‚  â”‚  âŒ NO full news articles            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ References to free sources
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FREE DATA SOURCES (Internet)                  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Alpaca Data APIâ”‚  â”‚   FRED API     â”‚               â”‚
â”‚  â”‚ (OHLCV - FREE) â”‚  â”‚ (Econ - FREE)  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Alpaca News APIâ”‚  â”‚  Yahoo Finance â”‚               â”‚
â”‚  â”‚ (News - FREE)  â”‚  â”‚  (OHLCV - FREE)â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Download on-demand for ML
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ML TRAINING ENVIRONMENT (Laptop)                â”‚
â”‚  (Temporary cache: ~500MB)                             â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  ML Training Script                   â”‚             â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚             â”‚
â”‚  â”‚  1. Query: trading_decisions         â”‚             â”‚
â”‚  â”‚  2. Download: Alpaca OHLCV (cache)   â”‚             â”‚
â”‚  â”‚  3. Calculate: Technical indicators  â”‚             â”‚
â”‚  â”‚  4. Fetch: News full text (cache)    â”‚             â”‚
â”‚  â”‚  5. Join: Create training dataset    â”‚             â”‚
â”‚  â”‚  6. Train: ML models                 â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Local Cache                          â”‚             â”‚
â”‚  â”‚  (~500MB, temporary)                  â”‚             â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚             â”‚
â”‚  â”‚  AAPL_2020-2025_5min.parquet         â”‚             â”‚
â”‚  â”‚  TSLA_2020-2025_5min.parquet         â”‚             â”‚
â”‚  â”‚  ...                                   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Data Flow Diagram

```
TRADING WORKFLOW (Production):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Scanner evaluates 100 candidates
2. Calculate composite scores
3. STORE decision in trading_decisions table
   - Symbol, timestamp, scores
   - REFERENCE: ohlcv_source = "Alpaca API endpoint"
4. Execute trades
5. STORE outcome in trade_outcomes table

STORAGE: ~5 decisions/day Ã— 500 bytes = 2.5 KB/day
         ~945 KB/year
         ~4.7 MB over 5 years

ML TRAINING WORKFLOW (Laptop, monthly):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Query: Get all trading_decisions (last 5 years)
2. Extract: Unique symbols + time ranges
3. Download: OHLCV from Alpaca API (parallel, 10-20 min)
4. Cache: Save to local parquet files (~500MB total)
5. Calculate: Technical indicators (5 min, CPU)
6. Fetch: News full text if needed (5 min, network)
7. Join: Create complete training dataset
8. Train: ML models (GPU, 10-30 min depending on model)

Total Time: 30-60 minutes first run, 10-20 min subsequent runs
```

---

## 7. Rate Limits & Scheduling

### 7.1 Alpaca API Rate Limits

```yaml
Free Tier:
  - 200 requests/minute
  - 1,000 bars per request
  
Example: Download 5 years AAPL 5-minute bars
  Total bars: ~262,000
  Requests needed: 262 requests (1,000 bars each)
  Time: 262 requests Ã· 200/min = 1.3 minutes
  
Example: Download 50 symbols Ã— 5 years
  Total requests: 50 Ã— 262 = 13,100 requests
  Time: 13,100 Ã· 200/min = 65 minutes (~1 hour)
  
Strategy: Download overnight, cache locally
```

### 7.2 Scheduled Download Strategy

```python
# ML training cron job (laptop)
# Run weekly to refresh cache

#!/usr/bin/env python3
"""
Weekly ML cache refresh
Downloads updated OHLCV data from Alpaca
Runs: Sunday 2:00 AM (when system idle)
"""

import asyncio
import pandas as pd
from datetime import datetime, timedelta

CACHE_DIR = "/home/claude/ml_training_cache"
SYMBOLS_TO_CACHE = 50  # Top 50 most-traded symbols

async def refresh_ml_cache():
    """Download latest data for ML training"""
    
    # Get list of symbols from trading_decisions
    symbols = await db.fetch("""
        SELECT DISTINCT s.symbol
        FROM trading_decisions td
        JOIN securities s ON s.security_id = td.security_id
        ORDER BY td.created_at DESC
        LIMIT $1
    """, SYMBOLS_TO_CACHE)
    
    # Download last 5 years for each symbol
    end_date = datetime.now()
    start_date = end_date - timedelta(days=5*365)
    
    for symbol in symbols:
        print(f"Downloading {symbol}...")
        
        # Download from Alpaca (respecting rate limits)
        data = await download_alpaca_ohlcv(
            symbol=symbol,
            timeframe="5Min",
            start=start_date,
            end=end_date
        )
        
        # Save to cache
        cache_file = f"{CACHE_DIR}/{symbol}_5min.parquet"
        data.to_parquet(cache_file)
        
        # Rate limit: sleep 0.3s between requests (200/min)
        await asyncio.sleep(0.3)
    
    print(f"Cache refreshed: {len(symbols)} symbols")

# Run weekly via cron
if __name__ == "__main__":
    asyncio.run(refresh_ml_cache())
```

**Cron Schedule**:
```bash
# Add to laptop crontab
# Run every Sunday at 2:00 AM

0 2 * * 0 /usr/bin/python3 /path/to/refresh_ml_cache.py >> /var/log/catalyst/ml_cache.log 2>&1
```

**Result**: Always have fresh data cached locally for instant ML training

---

## 8. Recommendations

### 8.1 Immediate Actions (Next 7 Days)

**1. Update Database Schema**
```sql
-- Deploy reference-based schema
-- Replace trading_history, technical_indicators, news_sentiment
-- With trading_decisions, trade_outcomes, news_references

Estimated time: 4 hours
Impact: 97.5% storage reduction
```

**2. Implement Reference Storage**
```python
# Update all services to store references, not raw data
# Scanner: Store decision + Alpaca API reference
# News: Store metadata + URL, not full text
# Technical: Don't store, calculate on-demand

Estimated time: 8 hours
Impact: Production system uses 5GB instead of 200GB
```

**3. Build ML Cache System**
```python
# Create download_alpaca_cached() function
# Create weekly refresh cron job
# Test ML training with on-demand downloads

Estimated time: 4 hours
Impact: ML training works with 30-min delay, 8x cost savings
```

### 8.2 Trade-offs Summary

| Approach | Storage | Cost (5yr) | ML Training Time | Complexity |
|----------|---------|------------|------------------|------------|
| **Full Storage** | 200 GB | $7,200 | 5 min (instant) | Low |
| **Reference-Based** | 5 GB | $900 | 30 min (first run) | Medium |
| **Hybrid (90-day + reference)** | 15 GB | $1,260 | 10 min | High |

### 8.3 Final Recommendation

**Use Reference-Based Storage** with these optimizations:

```yaml
Store in Database (5GB):
  âœ… trading_decisions (what we chose, why)
  âœ… trade_outcomes (ground truth)
  âœ… news_references (metadata + URLs)
  âœ… All trading operations
  
Reference from Free APIs:
  ðŸ“Ž OHLCV data â†’ Alpaca API (free)
  ðŸ“Ž Economic data â†’ FRED API (free)
  ðŸ“Ž News full text â†’ URLs (free)
  ðŸ“Ž Technical indicators â†’ Calculate from OHLCV
  
Cache Locally for ML (500MB):
  ðŸ’¾ Top 50 symbols Ã— 5 years OHLCV
  ðŸ’¾ Refresh weekly via cron
  ðŸ’¾ Instant ML training after first download
```

**Benefits**:
- Storage: 5GB (vs 200GB) âœ…
- Cost: $900 over 5 years (vs $7,200) âœ…
- ML capability: Full (30-min delay first run) âœ…
- Complexity: Medium (manageable) âœ…

**Net Result**: 
- Save $6,300 over 5 years
- Keep full ML training capability
- Add 25 minutes to first ML training run
- Subsequent runs: cached, instant

---

## 9. Conclusion

### 9.1 Your Insight Was Brilliant

You correctly identified that **most data is freely available** on the internet. Instead of:
- Storing 200GB locally (expensive)
- Paying $120/month for storage (wasteful)

We can:
- Store 5GB of decisions/outcomes (cheap)
- Pay $15/month for storage (efficient)
- Download source data on-demand for ML (free, slight delay)

### 9.2 Key Takeaway

```yaml
Don't store what's already stored elsewhere.
Store what's UNIQUE to your system:
  âœ… Your decisions
  âœ… Your outcomes
  âœ… Your composite scores
  âœ… Your lessons learned

Reference what's freely available:
  ðŸ“Ž OHLCV from Alpaca (free)
  ðŸ“Ž News from Alpaca/Yahoo (free)
  ðŸ“Ž Economic data from FRED (free)
  ðŸ“Ž Technical indicators (calculate on-demand)
```

**Result**: 97.5% storage savings, 0% ML capability loss.

---

**END OF FREE DATA SOURCES & ML STORAGE STRATEGY**

ðŸŽ© **DevGenius Status**: Reference-based storage designed! Store decisions, not data. Download free sources on-demand. Save $6,300 over 5 years with zero loss of ML training capability! ðŸš€
